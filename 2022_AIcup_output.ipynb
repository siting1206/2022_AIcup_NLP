{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkKL5L734UVtJ2URLXRnz1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siting1206/AIcup_NLP/blob/main/2022_AIcup_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBjaOwpUg9pz"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install pandas nltk\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1OQe_PHhD7P",
        "outputId": "65cf24d3-073a-4267-ff7d-e08ec6d3598d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "JMAjGT5nkCxn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "Vu8t3DROhIvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data file\n",
        "directory=\"/content/drive/MyDrive/AICUP_NLP/\"\n",
        "\n",
        "\n",
        "file=directory+\"Batch_answers - train_data (no-blank).csv\"\n",
        "df=pd.read_csv(file, encoding = \"ISO-8859-1\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ij5R3EM3hS3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['q','r']] = df[['q','r']].apply(lambda x: x.str.strip('\\\"'))\n",
        "df['r'] = df['s'] + ':' + df['r']\n",
        "test = df"
      ],
      "metadata": {
        "id": "BA6YdHUwhosX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "pWmRm1akjF6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "qeiHLgDCjFX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_q = df['q'].tolist()\n",
        "test_data_r = df['r'].tolist()\n",
        "test_encodings = tokenizer(test_data_q, test_data_r, truncation=True, padding=True)\n",
        "tokenizer.decode(test_encodings['input_ids'][0])"
      ],
      "metadata": {
        "id": "64z4SsAYirBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "FUlxI55VjQGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class qrDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)"
      ],
      "metadata": {
        "id": "UT_G7oJcjRnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = qrDataset(test_encodings)"
      ],
      "metadata": {
        "id": "zKNS0SgYjUNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(test_dataset))"
      ],
      "metadata": {
        "id": "YIltHQ2jjWqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "KvN1jkCijZT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "class myModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(myModel, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.fc = nn.Linear(768, 4)\n",
        "        \n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\n",
        "        logits = output[0]\n",
        "        out = self.fc(logits)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "WZkPx3iAjbQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set GPU / CPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# Put model on device\n",
        "model = myModel().to(device)"
      ],
      "metadata": {
        "id": "vAOBNl5Ujl42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "61mXt-IYjor2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = myModel().to(device)\n",
        "model.load_state_dict(torch.load(directory + 'aicup_model'))"
      ],
      "metadata": {
        "id": "ox8Y0akljuZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "30wj54GbktW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test_loader):\n",
        "    predict_pos = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    q_sub_output, r_sub_output = [],[]\n",
        "\n",
        "    loop = tqdm(test_loader, leave=True)\n",
        "    for batch_id, batch in enumerate(loop):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        token_type_ids = batch['token_type_ids'].to(device)\n",
        "\n",
        "        # model output\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        \n",
        "        q_start_logits, r_start_logits, q_end_logits, r_end_logits = torch.split(outputs, 1, 2)\n",
        "\n",
        "        q_start_logits = q_start_logits.squeeze(-1).contiguous()\n",
        "        r_start_logits = r_start_logits.squeeze(-1).contiguous()\n",
        "        q_end_logits = q_end_logits.squeeze(-1).contiguous()\n",
        "        r_end_logits = r_end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        q_start_prdict = torch.argmax(q_start_logits, 1).cpu().numpy()\n",
        "        r_start_prdict = torch.argmax(r_start_logits, 1).cpu().numpy()\n",
        "        q_end_prdict = torch.argmax(q_end_logits, 1).cpu().numpy()\n",
        "        r_end_prdict = torch.argmax(r_end_logits, 1).cpu().numpy()\n",
        "\n",
        "        for i in range(len(input_ids)):\n",
        "            predict_pos.append((q_start_prdict[i].item(), r_start_prdict[i].item(), q_end_prdict[i].item(), r_end_prdict[i].item()))\n",
        "\n",
        "            q_sub = tokenizer.decode(input_ids[i][q_start_prdict[i]:q_end_prdict[i]+1])\n",
        "            r_sub = tokenizer.decode(input_ids[i][r_start_prdict[i]:r_end_prdict[i]+1])\n",
        "            \n",
        "            q_sub_output.append(q_sub)\n",
        "            r_sub_output.append(r_sub)\n",
        "    \n",
        "    return q_sub_output, r_sub_output, predict_pos"
      ],
      "metadata": {
        "id": "3vUyO3nPkvoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_sub_output, r_sub_output, predict_pos = predict(test_loader)"
      ],
      "metadata": {
        "id": "2qoJ8kMwkw6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_post_fn(test, q_sub_output, r_sub_output):\n",
        "    q_sub, r_sub = [], []\n",
        "    for i in range(len(test)):\n",
        "\n",
        "        q_sub_pred = q_sub_output[i].split()\n",
        "        r_sub_pred = r_sub_output[i].split()\n",
        "\n",
        "        if q_sub_pred is None:\n",
        "            q_sub_pred = []\n",
        "        q_sub_error_index = q_sub_pred.index('[SEP]') if '[SEP]' in q_sub_pred else -1\n",
        "\n",
        "        if q_sub_error_index != -1:\n",
        "            q_sub_pred = q_sub_pred[:q_sub_error_index]\n",
        "\n",
        "        temp = r_sub_pred.copy()\n",
        "        if r_sub_pred is None:\n",
        "            r_sub_pred = []\n",
        "        else:\n",
        "            for j in range(len(temp)):\n",
        "                if temp[j] == '[SEP]':\n",
        "                    r_sub_pred.remove('[SEP]')\n",
        "                if temp[j] == '[PAD]':\n",
        "                    r_sub_pred.remove('[PAD]')\n",
        "\n",
        "        q_sub.append(' '.join(q_sub_pred))\n",
        "        r_sub.append(' '.join(r_sub_pred))\n",
        "\n",
        "    return q_sub, r_sub"
      ],
      "metadata": {
        "id": "lstTv77kkyG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_sub, r_sub = get_output_post_fn(test, q_sub_output, r_sub_output)"
      ],
      "metadata": {
        "id": "yB9rWkwxk0Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['q_sub'] = q_sub\n",
        "test['r_sub'] = r_sub"
      ],
      "metadata": {
        "id": "59uL0UnNk3UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "-hnEPbEBlNnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test."
      ],
      "metadata": {
        "id": "hr9ShXhCmxfe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}